{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226b4316-9fd1-46f4-a876-6b3b4ddb61d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d652c9c-4246-42cb-b813-cb4fe62915d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get or create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual DataFrame loading logic)\n",
    "data = [\n",
    "    (1, 25, 50000.0, \"Male\", \"New York\"),\n",
    "    (2, 30, 60000.0, \"Female\", \"San Francisco\"),\n",
    "    (3, 45, 80000.0, \"Male\", \"Chicago\"),\n",
    "    (4, 22, 45000.0, \"Female\", \"Boston\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"age\", \"income\", \"gender\", \"city\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c7a47e-5574-42e6-9bce-82cf6ee1e1d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902a66b6-9135-4399-b33d-d1ac56263e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get column names as a comma-separated string\n",
    "col_names = \", \".join(df.columns)\n",
    "print(\"Column names: \" + col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2e8127-70fe-4075-8974-a4a33bbb8e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the prompt for the LLM\n",
    "prompt = f\"\"\"\n",
    "Given a dataset with the following column names: {col_names}.\n",
    "\n",
    "Suggest 5 feature engineering ideas to improve machine learning model performance. For each idea:\n",
    "- Describe the new feature.\n",
    "- Explain why it might be useful.\n",
    "- Provide the PySpark code snippet to create it (using functions like withColumn, when, etc.).\n",
    "\n",
    "Output the suggestions in a numbered list.\n",
    "\"\"\"\n",
    "\n",
    "# Run SQL query with ai_query to get suggestions\n",
    "suggestions_df = spark.sql(f\"\"\"\n",
    "SELECT ai_query(\n",
    "  'databricks-llama-4-maverick',\n",
    "  '{prompt}'\n",
    ") AS feature_suggestions\n",
    "\"\"\")\n",
    "\n",
    "# Collect and print the result (since it's a single row)\n",
    "suggestions = suggestions_df.collect()[0][\"feature_suggestions\"]\n",
    "print(\"LLM Suggested Feature Engineering Ideas:\\n\")\n",
    "print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ba58cb-9b0d-46e8-bc03-d3522fa4c470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example implementation based on LLM suggestion (replace with actual suggestions)\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_engineered = df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "    .when((col(\"age\") >= 30) & (col(\"age\") < 50), \"Adult\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "df_engineered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5e475c-2d19-4dab-8c19-bfbd2101f114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_engineered = df_engineered.withColumn(\"income_bracket\", \n",
    "                   F.when(F.col(\"income\") < 30000, \"Low\")\n",
    "                   .when((F.col(\"income\") >= 30000) & (F.col(\"income\") < 60000), \"Medium\")\n",
    "                   .otherwise(\"High\"))\n",
    "\n",
    "df_engineered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99768767-2483-4822-891a-ea046c485aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_engineered = df_engineered.withColumn(\"age_income_interaction\", F.col(\"age\") * F.col(\"income\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65b7387-a62b-44d3-993e-d1f9137b91a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_engineered = df_engineered.withColumn(\"gender_age_group\", F.concat(F.col(\"gender\"), F.lit(\"_\"), F.col(\"age_group\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d877d1a1-39f8-4255-80ae-7d795c0c5ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_engineered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a2ff9a-6c72-4c7a-9216-b224c0411ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) genai_automated_feature_eng",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
